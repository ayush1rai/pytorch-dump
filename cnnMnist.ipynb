{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6fyWBpP-59Xz"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOLGn_UXG5-t"
      },
      "source": [
        "### Making NN and CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DUxWr06qG3ip"
      },
      "outputs": [],
      "source": [
        "class NN(nn.Module):\n",
        "  def __init__(self, input_size, num_classes):\n",
        "    super(NN, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, 50)\n",
        "    self.fc2 = nn.Linear(50, num_classes)\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hmWZxlZ4QOyn"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, in_channels = 1, num_classes = 10):\n",
        "    super(CNN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1))\n",
        "    self.pool = nn.MaxPool2d(kernel_size =(2, 2), stride = (2,2))\n",
        "    self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size =(3, 3), stride = (1, 1), padding = (1, 1))\n",
        "    self.fc = nn.Linear(7*7*16, num_classes)\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "s8HXzZyvSj7E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ba2913e-c628-45bb-e5f8-191ab9eee637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1491: FutureWarning: The repository for mnist contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mnist\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('mnist')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PooExRs8TO6d",
        "outputId": "576dc112-2a06-4d03-e52a-e7102b0b9c44"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['image', 'label'],\n",
              "        num_rows: 60000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['image', 'label'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FDhm4S8rTZrX"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "class MNISTDataset(Dataset):\n",
        "    def __init__(self, hf_dataset, transform=None):\n",
        "        self.dataset = hf_dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.dataset[idx]['image']\n",
        "        label = self.dataset[idx]['label']\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iYdTAGxlUvOU"
      },
      "outputs": [],
      "source": [
        "train_dataset = MNISTDataset(dataset['train'], transform=transform)\n",
        "test_dataset = MNISTDataset(dataset['test'], transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Sgr7IiZRYJLK"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8vdsrcqqamp0"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "in_channel = 1\n",
        "num_channels = 10\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "num_epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ct72fVifbmWH"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
        "test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XRZIe7tdZa14"
      },
      "outputs": [],
      "source": [
        "model = CNN().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Gmn6uXspcwoR"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "q08XNzKqd0xp"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "  for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "    data = data.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    scores = model(data)\n",
        "    loss = criterion(scores, targets)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNmuKUKfgn_5",
        "outputId": "a8ad0677-f83b-45f0-b5dc-275e25fb68d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking accuracy on training data\n",
            "Got 59196 / 60000 with accuracy 98.66\n",
            "Checking accuracy on test data\n",
            "Got 9851 / 10000 with accuracy 98.51\n"
          ]
        }
      ],
      "source": [
        "def check_accuracy(loader, model):\n",
        "  if loader.dataset == train_dataset:\n",
        "    print(\"Checking accuracy on training data\")\n",
        "  else:\n",
        "    print(\"Checking accuracy on test data\")\n",
        "  num_correct = 0\n",
        "  num_samples = 0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x, y in loader:\n",
        "      x = x.to(device = device)\n",
        "      y = y.to(device = device)\n",
        "\n",
        "      scores = model(x)\n",
        "      _, predictions = scores.max(1)\n",
        "      num_correct += (predictions == y).sum()\n",
        "      num_samples += predictions.size(0)\n",
        "\n",
        "    print(f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}\")\n",
        "  model.train()\n",
        "\n",
        "check_accuracy(train_loader, model)\n",
        "check_accuracy(test_loader, model)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLw5nKsNcsBle8sV2feIb1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}